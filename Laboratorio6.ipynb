{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfEKuvL1W/uv0gUiThYccP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HectorDelgadoJ/Laboratorio-6/blob/main/Laboratorio6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 6\n"
      ],
      "metadata": {
        "id": "sPXZ2Y_GDuma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta práctica, implementaremos y validaremos el Clasificador de la Distancia Mínima, un modelo de aprendizaje supervisado basado en la asignación de muestras a la clase más cercana según la distancia euclidiana a los centroides de cada clase. Este clasificador es eficiente y adecuado para datos donde las clases están claramente diferenciadas en el espacio de características.\n",
        "Usaremos tres datasets de scikit-learn (Iris, Wine y Digits) y evaluaremos el rendimiento del clasificador mediante tres métodos de validación: Hold-Out estratificado (70/30), 10-Fold Cross-Validation estratificado, y Leave-One-Out. Cada método de validación nos permitirá observar el comportamiento y la precisión del modelo bajo diferentes enfoques de evaluación. Los resultados obtenidos brindarán una visión clara sobre la robustez y capacidad del clasificador para generalizar en estos conjuntos de datos."
      ],
      "metadata": {
        "id": "5fvsY_qXZi6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##* Paso 1: Importar Bibliotecas Necesarias"
      ],
      "metadata": {
        "id": "dyGGDRhRZoTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris, load_wine, load_digits  # Cargar datasets de ejemplo\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "3Ms_Pdp0FZaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## * Paso 2: Implementar el Clasificador de la Distancia Mínima\n",
        "El clasificador calculará el centroide de cada clase en el conjunto de entrenamiento y asignará cada punto de prueba a la clase más cercana (por distancia euclidiana)."
      ],
      "metadata": {
        "id": "6Pt2vqlDZ9Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinimumDistanceClassifier:\n",
        "    def fit(self, X, y):\n",
        "        # Calcular el centroide de cada clase\n",
        "        self.centroids = {}\n",
        "        for label in np.unique(y):\n",
        "            self.centroids[label] = X[y == label].mean(axis=0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Asignar cada muestra a la clase con el centroide más cercano\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            distances = {label: np.linalg.norm(sample - centroid) for label, centroid in self.centroids.items()}\n",
        "            predictions.append(min(distances, key=distances.get))\n",
        "        return np.array(predictions)\n"
      ],
      "metadata": {
        "id": "vEVi-2_maEJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## * Paso 3: Definir Función para Validación y Evaluación\n",
        "Esta función recibe el dataset, el clasificador, y el método de validación a aplicar."
      ],
      "metadata": {
        "id": "u3TYtpjTaHvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(X, y, validation_method):\n",
        "    clf = MinimumDistanceClassifier()\n",
        "    accuracies = []\n",
        "\n",
        "    if validation_method == \"Hold-Out\":\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    elif validation_method == \"10-Fold\":\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    elif validation_method == \"Leave-One-Out\":\n",
        "        loo = LeaveOneOut()\n",
        "        for train_index, test_index in loo.split(X):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    return np.mean(accuracies)\n"
      ],
      "metadata": {
        "id": "f8wmVyEzaMkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## * Paso 4: Cargar Datasets y Validar\n",
        "Usaremos tres datasets de scikit-learn: iris, wine, y digits. Para cada dataset, evaluaremos el modelo usando los tres métodos de validación."
      ],
      "metadata": {
        "id": "d4ZLDmXOaQXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datasets\n",
        "datasets = {\n",
        "    \"Iris\": load_iris(),\n",
        "    \"Wine\": load_wine(),\n",
        "    \"Digits\": load_digits()\n",
        "}\n",
        "\n",
        "# Validar el modelo con cada dataset y método de validación\n",
        "validation_methods = [\"Hold-Out\", \"10-Fold\", \"Leave-One-Out\"]\n",
        "\n",
        "for dataset_name, data in datasets.items():\n",
        "    X, y = data.data, data.target\n",
        "    print(f\"Results for {dataset_name} dataset:\")\n",
        "    for method in validation_methods:\n",
        "        accuracy = validate_model(X, y, method)\n",
        "        print(f\"{method} Accuracy: {accuracy:.4f}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "Ye9wCciBaWbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "El codigo coompleto:"
      ],
      "metadata": {
        "id": "PiGN5qfoaYy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "from sklearn.datasets import load_iris, load_wine, load_digits  # Datasets de ejemplo\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Definir el Clasificador de la Distancia Mínima\n",
        "class MinimumDistanceClassifier:\n",
        "    def fit(self, X, y):\n",
        "        # Calcular el centroide de cada clase\n",
        "        self.centroids = {}\n",
        "        for label in np.unique(y):\n",
        "            self.centroids[label] = X[y == label].mean(axis=0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Asignar cada muestra a la clase con el centroide más cercano\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            distances = {label: np.linalg.norm(sample - centroid) for label, centroid in self.centroids.items()}\n",
        "            predictions.append(min(distances, key=distances.get))\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Definir la función de validación\n",
        "def validate_model(X, y, validation_method):\n",
        "    clf = MinimumDistanceClassifier()\n",
        "    accuracies = []\n",
        "\n",
        "    if validation_method == \"Hold-Out\":\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    elif validation_method == \"10-Fold\":\n",
        "        skf = StratifiedKFold(n_splits=10)\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    elif validation_method == \"Leave-One-Out\":\n",
        "        loo = LeaveOneOut()\n",
        "        for train_index, test_index in loo.split(X):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    return np.mean(accuracies)\n",
        "\n",
        "# Cargar datasets\n",
        "datasets = {\n",
        "    \"Iris\": load_iris(),\n",
        "    \"Wine\": load_wine(),\n",
        "    \"Digits\": load_digits()\n",
        "}\n",
        "\n",
        "# Validar el modelo con cada dataset y método de validación\n",
        "validation_methods = [\"Hold-Out\", \"10-Fold\", \"Leave-One-Out\"]\n",
        "\n",
        "for dataset_name, data in datasets.items():\n",
        "    X, y = data.data, data.target\n",
        "    print(f\"Results for {dataset_name} dataset:\")\n",
        "    for method in validation_methods:\n",
        "        accuracy = validate_model(X, y, method)\n",
        "        print(f\"{method} Accuracy: {accuracy:.4f}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "0djHUtlDavNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2\n",
        "En esta segunda parte de la práctica, implementaremos y validaremos el Clasificador de 1-Vecino Más Cercano (1NN). Este método de clasificación supervisada asigna cada muestra a la clase de su vecino más cercano en el espacio de características, utilizando la distancia euclidiana para medir la similitud. La simplicidad y efectividad de este clasificador lo convierten en una opción ampliamente usada para problemas de clasificación donde las clases se encuentran bien definidas y separadas.\n",
        "Para evaluar el rendimiento del modelo, utilizaremos tres métodos de validación: Hold-Out (70/30) estratificado, 10-Fold Cross-Validation estratificado, y Leave-One-Out. Además, mediremos el desempeño del clasificador en términos de precisión (accuracy) y generaremos matrices de confusión, lo cual permitirá un análisis más profundo del comportamiento del modelo en diferentes datasets (Iris, Wine y Digits). Estos resultados nos ayudarán a comparar la efectividad del Clasificador 1NN y a explorar su capacidad de generalización en distintos conjuntos de datos."
      ],
      "metadata": {
        "id": "7cmkdzlBawH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## * Paso 1: Implementación del Clasificador 1NN\n"
      ],
      "metadata": {
        "id": "nyjb59WpcDx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "from sklearn.datasets import load_iris, load_wine, load_digits  # Datasets de ejemplo\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Definir el Clasificador 1NN\n",
        "class OneNearestNeighborClassifier:\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            # Calcular la distancia de la muestra a todos los puntos de entrenamiento\n",
        "            distances = np.linalg.norm(self.X_train - sample, axis=1)\n",
        "            # Encontrar el índice de la muestra de entrenamiento más cercana\n",
        "            nearest_neighbor_index = np.argmin(distances)\n",
        "            # Asignar la clase de la muestra de entrenamiento más cercana\n",
        "            predictions.append(self.y_train[nearest_neighbor_index])\n",
        "        return np.array(predictions)\n"
      ],
      "metadata": {
        "id": "GvLjHVVFcC90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## * Paso 2: Validación con el Método Hold-Out\n"
      ],
      "metadata": {
        "id": "ZaY_MDCIcMgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para validación y cálculo de precisión\n",
        "def validate_1NN_hold_out(X, y):\n",
        "    clf = OneNearestNeighborClassifier()\n",
        "    # Separar el conjunto de datos en entrenamiento (70%) y prueba (30%) estratificado\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calcular medidas de desempeño\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return accuracy, conf_matrix\n"
      ],
      "metadata": {
        "id": "Qtyee5hscScu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## * Paso 3: Probar con un Dataset (Iris) y Ver Resultados"
      ],
      "metadata": {
        "id": "s6lVK8Z0cU1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importar load_iris\n",
        "from sklearn.datasets import load_iris # Import the load_iris function\n",
        "\n",
        "# Cargar dataset Iris\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Validar usando Hold-Out y mostrar resultados\n",
        "accuracy, conf_matrix = validate_1NN_hold_out(X, y)\n",
        "print(\"Hold-Out Validation (Iris Dataset):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "7C7G_LtmcdTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código completo:"
      ],
      "metadata": {
        "id": "bTCYzdwEc3xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "from sklearn.datasets import load_iris, load_wine, load_digits\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Definir el Clasificador 1NN\n",
        "class OneNearestNeighborClassifier:\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            # Calcular la distancia euclidiana entre el sample y cada muestra de X_train\n",
        "            distances = np.linalg.norm(self.X_train - sample, axis=1)\n",
        "            # Encontrar el índice de la muestra más cercana\n",
        "            nearest_neighbor_index = np.argmin(distances)\n",
        "            # Asignar la clase de la muestra más cercana\n",
        "            predictions.append(self.y_train[nearest_neighbor_index])\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Función de validación para Hold-Out\n",
        "def validate_1NN_hold_out(X, y):\n",
        "    clf = OneNearestNeighborClassifier()\n",
        "    # División 70/30 con estratificación\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    return accuracy, conf_matrix\n",
        "\n",
        "# Función de validación para 10-Fold Cross-Validation\n",
        "def validate_1NN_k_fold(X, y, k=10):\n",
        "    clf = OneNearestNeighborClassifier()\n",
        "    skf = StratifiedKFold(n_splits=k)\n",
        "\n",
        "    accuracies = []\n",
        "    # Matriz de confusión acumulativa\n",
        "    confusion_matrices = np.zeros((len(np.unique(y)), len(np.unique(y))), dtype=int)\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "        confusion_matrices += confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    avg_accuracy = np.mean(accuracies)\n",
        "    return avg_accuracy, confusion_matrices\n",
        "\n",
        "# Función de validación para Leave-One-Out\n",
        "def validate_1NN_leave_one_out(X, y):\n",
        "    clf = OneNearestNeighborClassifier()\n",
        "    loo = LeaveOneOut()\n",
        "\n",
        "    accuracies = []\n",
        "    confusion_matrices = np.zeros((len(np.unique(y)), len(np.unique(y))), dtype=int)\n",
        "\n",
        "    for train_index, test_index in loo.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))\n",
        "        confusion_matrices += confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    avg_accuracy = np.mean(accuracies)\n",
        "    return avg_accuracy, confusion_matrices\n",
        "\n",
        "# Ejecutar validaciones con tres datasets diferentes\n",
        "datasets = {\n",
        "    \"Iris\": load_iris(),\n",
        "    \"Wine\": load_wine(),\n",
        "    \"Digits\": load_digits()\n",
        "}\n",
        "\n",
        "# Aplicar el clasificador y métodos de validación a cada dataset\n",
        "for dataset_name, dataset in datasets.items():\n",
        "    X, y = dataset.data, dataset.target\n",
        "    print(f\"\\nResults for {dataset_name} Dataset:\")\n",
        "\n",
        "    # Hold-Out Validation\n",
        "    accuracy, conf_matrix = validate_1NN_hold_out(X, y)\n",
        "    print(\"\\nHold-Out Validation:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # 10-Fold Cross-Validation\n",
        "    accuracy, conf_matrix = validate_1NN_k_fold(X, y)\n",
        "    print(\"\\n10-Fold Cross-Validation:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "    # Leave-One-Out Validation\n",
        "    accuracy, conf_matrix = validate_1NN_leave_one_out(X, y)\n",
        "    print(\"\\nLeave-One-Out Validation:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "72M37_Wvc7aE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}